{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T12:56:31.231766Z",
     "start_time": "2021-06-06T12:56:26.711955Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade tables\n",
    "#!pip install tqdm\n",
    "!pip install --upgrade tensorflow-addons\n",
    "#!pip install --upgrade conda \n",
    "#!pip install --upgrade tensorflow_probability==0.11.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T12:56:34.097574Z",
     "start_time": "2021-06-06T12:56:31.247550Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import scipy\n",
    "#import scipy.signal\n",
    "#from scipy.io import wavfile as wav\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "print(\"Tensorflow Version\")\n",
    "print(tf.__version__)\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T12:56:34.109643Z",
     "start_time": "2021-06-06T12:56:34.105156Z"
    }
   },
   "outputs": [],
   "source": [
    "# About Data & Refining\n",
    "sample_rate = 16000\n",
    "\n",
    "# Frame Length & Stride (sec)\n",
    "frame_length = 0.032\n",
    "frame_stride = 0.016\n",
    "\n",
    "# Mel filter bands for VNR\n",
    "mel_band_counts = 32\n",
    "\n",
    "# N-FFT & N-Hop & N-Mels\n",
    "NFFT = int(frame_length * sample_rate)\n",
    "NHOP = int(frame_stride * sample_rate)\n",
    "NMEL = int(mel_band_counts)\n",
    "\n",
    "print('N-FFT:', NFFT)\n",
    "print('N-HOP:', NHOP)\n",
    "print('N-Mel:', NMEL)\n",
    "\n",
    "# Window Function Decay (exponential) 0.3sec/60dB\n",
    "TAU = -((0.3*sample_rate)-1) / np.log(0.001)\n",
    "WINDOW_FN = scipy.signal.get_window(('exponential', 0, TAU), NFFT)\n",
    "WINDOW_LENGTH_TIME = 0.2 # sec\n",
    "WINDOW_LENGTH = int(WINDOW_LENGTH_TIME * sample_rate)\n",
    "print(WINDOW_LENGTH)\n",
    "print('Exponential Decay - Tau:', TAU)\n",
    "\n",
    "# Bandpass filter freq range\n",
    "BANDPASS_RANGE = [150, 5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T13:12:30.043247Z",
     "start_time": "2021-05-16T13:12:29.948253Z"
    }
   },
   "outputs": [],
   "source": [
    "TARGET_DIR = [\n",
    "    'clean',\n",
    "    'noise',\n",
    "    'noisy',\n",
    "]\n",
    "\n",
    "class DatasetFiles:\n",
    "    def __init__(self, files, labels, \n",
    "                 sample_rate=None, n_fft=NFFT, n_hop=NHOP, \n",
    "                 window_fn=WINDOW_FN, win_length = WINDOW_LENGTH,\n",
    "                 bandpass_range=BANDPASS_RANGE, stft_center=True, **kwargs):\n",
    "        self._data = { k: v for k, v in zip(labels, files) }\n",
    "        self.n_fft = n_fft\n",
    "        self.n_hop = n_hop\n",
    "        self.sample_rate = sample_rate\n",
    "        self.window_fn = window_fn\n",
    "        self.win_length = win_length\n",
    "        self.bandpass_range = bandpass_range\n",
    "        self.center = stft_center\n",
    "    \n",
    "    def process_to_list(self):\n",
    "        vad, vnr = self.process_targets()\n",
    "        \n",
    "        inputs = self.process_input()\n",
    "        \n",
    "        return vad.tolist(), vnr.tolist(), inputs.tolist()\n",
    "        \n",
    "        \n",
    "    def process_targets(self):\n",
    "        x = self._stft(self.clean)\n",
    "        v = self._stft(self.noise)\n",
    "        \n",
    "        vad = self._vad(x)\n",
    "        \n",
    "        vnr = self._vnr(x, v)\n",
    "\n",
    "        return vad, vnr\n",
    "    \n",
    "    def process_input(self, n_mels=64):\n",
    "        _input = self._input(n_mels)\n",
    "        \n",
    "        return _input.T\n",
    "        \n",
    "    \n",
    "    def _input(self, n_mels=64):  \n",
    "        mel = librosa.feature.melspectrogram(self.noisy, sr=self.sr, n_mels=n_mels,\n",
    "                                             n_fft=self.n_fft, hop_length=self.n_hop)\n",
    "        \n",
    "        return librosa.core.power_to_db(mel)\n",
    "\n",
    "        \n",
    "    def _vad(self, x_spectrogram):\n",
    "        t_wx = self._bandpass_stft(x_spectrogram)\n",
    "        \n",
    "        amplitude = np.abs(t_wx) ** 2\n",
    "        \n",
    "        mean_frame = amplitude.mean(axis=1)\n",
    "        \n",
    "        max_frame = mean_frame.max()\n",
    "        \n",
    "        return np.array(mean_frame > (max_frame*0.01)).astype(float)\n",
    "        \n",
    "    def _vnr(self, x_spec, v_spec, db_range=(-15, 40)):\n",
    "        melx = self._mel(x_spec, power=1)\n",
    "        melv = self._mel(v_spec, power=1)\n",
    "        \n",
    "        x = (np.abs(melx)**2).mean(axis=0)\n",
    "        v = (np.abs(melv)**2).mean(axis=0)\n",
    "        \n",
    "        result = []\n",
    "        for a, b in zip(x, v):\n",
    "            if b == 0:  # to avoide zero divide error\n",
    "                result.append(np.float64(db_range[1]))\n",
    "            else:\n",
    "                z = np.divide(a, b)\n",
    "                \n",
    "                if z != 0:\n",
    "                    result.append(np.log10(z) * 10)\n",
    "                else:\n",
    "                    result.append(np.float64(db_range[1]))\n",
    "                    \n",
    "        return np.interp(np.array(result).clip(*db_range), db_range, (0, 1))\n",
    "    \n",
    "    def _stft(self, signal, window_fn='hann', window_length=None):        \n",
    "        # stft (Center = False)\n",
    "        spectrogram = librosa.stft(signal, n_fft=self.n_fft, hop_length=self.n_hop,\n",
    "                                   window =window_fn,\n",
    "                                   win_length=window_length,\n",
    "                                   center=self.center)\n",
    "\n",
    "        return spectrogram # (bins, frames)\n",
    "    \n",
    "    def _mel(self, spectrogram, n_mels=32, power=2):\n",
    "        # Mel Spectrogram\n",
    "        mels = librosa.feature.melspectrogram(S=spectrogram, sr=self.sr, power=power, \n",
    "                                              n_fft=self.n_fft, n_mels=n_mels)\n",
    "        return mels\n",
    "    \n",
    "    def _bandpass_stft(self, spectrogram, clip=False):\n",
    "        start, end = self.bandpass_range\n",
    "        \n",
    "        if start is None:\n",
    "            start = 0\n",
    "        if end is None:\n",
    "            end = self.sr // 2\n",
    "\n",
    "        frequency_map = librosa.fft_frequencies(self.sr, self.n_fft)\n",
    "        \n",
    "        bandpass_filter = np.logical_and(start < frequency_map, frequency_map < end)\n",
    "        \n",
    "        if clip:\n",
    "            bandpass_clip = [idx for idx, i in enumerate(bandpass_filter) if i > 0]\n",
    "            \n",
    "            return spectrogram[:,bandpass_clip]\n",
    "        else:\n",
    "            return (bandpass_filter * spectrogram.T)\n",
    "        \n",
    "    @property\n",
    "    def clean(self):\n",
    "        return librosa.load(self.clean_path, sr=self.sr)[0]\n",
    "    \n",
    "    @property\n",
    "    def noise(self):\n",
    "        return librosa.load(self.noise_path, sr=self.sr)[0]\n",
    "    \n",
    "    @property\n",
    "    def noisy(self):\n",
    "        return librosa.load(self.noisy_path, sr=self.sr)[0]\n",
    "   \n",
    "    @property\n",
    "    def clean_path(self):\n",
    "        return self._data['clean']\n",
    "        \n",
    "    @property\n",
    "    def noise_path(self):\n",
    "        return self._data['noise']\n",
    "        \n",
    "    @property\n",
    "    def noisy_path(self):\n",
    "        return self._data['noisy']\n",
    "    \n",
    "    @property\n",
    "    def sr(self):\n",
    "        return self.sample_rate\n",
    "    \n",
    "class Dataset:\n",
    "    def __init__(self, root_dir, labels, validate=True, **kwargs):\n",
    "        self.rootdir = root_dir\n",
    "        self.labels = labels\n",
    "        self.files = self._load_files(**kwargs)\n",
    "            \n",
    "    def _load_files(self, **kwargs):\n",
    "        datafiles = [] # [{file_id: 'file_path', ...}, ...]\n",
    "        for i in self.labels:\n",
    "            path = os.path.join(self.rootdir, i)\n",
    "            files = { k: v for k, v in sorted(self._scandir(path).items()) } # file_id: file_path\n",
    "            datafiles.append(files)\n",
    "            \n",
    "        datafiles = [DatasetFiles(files, self.labels, **kwargs) for files in zip(*(i.values() for i in self._validate(datafiles)))]\n",
    "            \n",
    "        return datafiles\n",
    "    \n",
    "    def _scandir(self, path):\n",
    "        return { self._fn_parser(i)[0]: i for i in os.scandir(path) }\n",
    "        \n",
    "    @staticmethod\n",
    "    def _fn_parser(fn):\n",
    "        name, ext = os.path.splitext(fn)\n",
    "        name, file_id = name.rsplit('_', 1)\n",
    "        return int(file_id), fn\n",
    "    \n",
    "    @staticmethod\n",
    "    def _validate(datalists):\n",
    "        _cache = None\n",
    "        for idx, i in enumerate(datalists):\n",
    "            if idx > 0:\n",
    "                if (_cache != set(i)):\n",
    "                    raise ValueError('Dataset Unmatched!!')\n",
    "            _cache = set(i)\n",
    "        print('Validate Complete')\n",
    "        return datalists\n",
    "    \n",
    "dataset_root = \"./DNS-Challenge-ds/dataset-50h\"\n",
    "\n",
    "dataset = Dataset(dataset_root, TARGET_DIR, sample_rate=16000)\n",
    "\n",
    "print(\"Total\", len(dataset.files), \"data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T10:54:03.758124Z",
     "start_time": "2021-05-16T10:54:00.196109Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# example\n",
    "\n",
    "example = dataset.files[0]\n",
    "\n",
    "targets = example.process_targets()\n",
    "inputs = example.process_input()\n",
    "\n",
    "fig, axs = plt.subplots(3)\n",
    "fig.suptitle('Clean, VAD, VNR')\n",
    "\n",
    "fig.set_figheight(8)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "\n",
    "axs[0].plot(example.clean)\n",
    "axs[1].plot(targets[0])\n",
    "axs[2].plot(targets[1])\n",
    "plt.show()\n",
    "\n",
    "print(inputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T13:21:08.776542Z",
     "start_time": "2021-05-16T13:18:12.165743Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessig VAD, VNR\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import multiprocessing\n",
    "from multiprocessing import Lock\n",
    "\n",
    "process_worker_count = 10\n",
    "\n",
    "manager = multiprocessing.Manager()\n",
    "result_list = manager.list()\n",
    "\n",
    "data = dataset.files\n",
    "\n",
    "lock = Lock()\n",
    "\n",
    "def worker(l, data):\n",
    "    for idx, item in enumerate(data):\n",
    "        result_list.append(item.process_to_list())\n",
    "        l.acquire()\n",
    "        try:\n",
    "            print(\"Processing file \", idx+1,\"/\",len(data))\n",
    "        finally:\n",
    "            l.release()\n",
    "        \n",
    "process_pool = []\n",
    "for i in range(process_worker_count):\n",
    "    start = i*(len(data)//process_worker_count)\n",
    "    end = (i+1)*(len(data)//process_worker_count)\n",
    "    process_pool.append(\n",
    "        multiprocessing.Process(target=worker, args=[lock, data[start:end]])\n",
    "    )\n",
    "\n",
    "for process in process_pool:\n",
    "    process.start()\n",
    "    \n",
    "for process in process_pool:\n",
    "    process.join()\n",
    "\n",
    "print(len(result_list))\n",
    "\n",
    "df = pd.DataFrame(result_list, columns=('VAD', 'VNR', 'INPUT'))\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T22:55:28.358374Z",
     "start_time": "2021-05-13T22:55:27.526624Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save Dataset\n",
    "\n",
    "df.head(10)\n",
    "\n",
    "df.to_pickle('dataset_32.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T22:50:03.335990Z",
     "start_time": "2021-05-15T22:49:59.686537Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read Dataset\n",
    "\n",
    "df = pd.read_pickle('dataset.pk')\n",
    "\n",
    "df.head(10)\n",
    "\n",
    "print(\"Data Count:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T22:50:06.447469Z",
     "start_time": "2021-05-15T22:50:05.183925Z"
    }
   },
   "outputs": [],
   "source": [
    "test_ratio = 0.1\n",
    "\n",
    "validation_ratio = 0.2\n",
    "\n",
    "test_count  = int(len(df)*test_ratio)\n",
    "\n",
    "valid_count = int(len(df)*validation_ratio)\n",
    "\n",
    "\n",
    "# split dataframe\n",
    "df_test = df.iloc[:test_count, :]\n",
    "\n",
    "df_valid = df.iloc[test_count:valid_count, :]\n",
    "\n",
    "df_train = df.iloc[valid_count:, :]\n",
    "\n",
    "\n",
    "# splist datset\n",
    "vad_test , vnr_test , x_test  = [np.array(i) for i in zip(*df_test.to_numpy())]\n",
    "\n",
    "vad_valid, vnr_valid, x_valid = [np.array(i) for i in zip(*df_valid.to_numpy())]\n",
    "\n",
    "vad_train, vnr_train, x_train = [np.array(i) for i in zip(*df_train.to_numpy())]\n",
    "\n",
    "# print('Sample Shape is ', x_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T22:27:40.327055Z",
     "start_time": "2021-05-15T22:27:39.388152Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "\n",
    "class AutoClipper:\n",
    "    def __init__(self, clip_percentile, history_size=10000):\n",
    "        self.clip_percentile = clip_percentile\n",
    "        self.grad_history = tf.Variable(tf.zeros(history_size), trainable=False)\n",
    "        self.i = tf.Variable(0, trainable=False)\n",
    "        self.history_size = history_size\n",
    "\n",
    "    def __call__(self, grads_and_vars):\n",
    "        grad_norms = [self._get_grad_norm(g) for g, _ in grads_and_vars]\n",
    "        total_norm = tf.norm(grad_norms)\n",
    "        assign_idx = tf.math.mod(self.i, self.history_size)\n",
    "        self.grad_history = self.grad_history[assign_idx].assign(total_norm)\n",
    "        self.i = self.i.assign_add(1)\n",
    "        clip_value = tfp.stats.percentile(self.grad_history[: self.i], q=self.clip_percentile)\n",
    "        return [(tf.clip_by_norm(g, clip_value), v) for g, v in grads_and_vars]\n",
    "\n",
    "    def _get_grad_norm(self, t, axes=None, name=None):\n",
    "        values = tf.convert_to_tensor(t.values if isinstance(t, tf.IndexedSlices) else t, name=\"t\")\n",
    "\n",
    "        # Calculate L2-norm, clip elements by ratio of clip_norm to L2-norm\n",
    "        l2sum = tf.math.reduce_sum(values * values, axes, keepdims=True)\n",
    "        pred = l2sum > 0\n",
    "        # Two-tap tf.where trick to bypass NaN gradients\n",
    "        l2sum_safe = tf.where(pred, l2sum, tf.ones_like(l2sum))\n",
    "        return tf.squeeze(tf.where(pred, tf.math.sqrt(l2sum_safe), l2sum))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T23:39:39.783253Z",
     "start_time": "2021-05-15T23:39:39.585841Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "_N_FRAMES = 1874\n",
    "\n",
    "# about AdamW\n",
    "_ADAM_W_LR = 0.00005\n",
    "_ADAM_W_WD = 0.01\n",
    "\n",
    "# about loss functions\n",
    "alpha = 0.2\n",
    "\n",
    "lossVADVNR1 = ['binary_crossentropy', 'mae']\n",
    "lossVADVNR1_weight = [1-alpha, alpha]\n",
    "\n",
    "def lossSUM2BCE(y_true, y_pred):\n",
    "    tvad, tvnr = tf.split(y_true, [1, 1], axis=-1)\n",
    "    pvad, pvnr = tf.split(y_pred, [1, 1], axis=-1)\n",
    "    \n",
    "    bce = tf.keras.losses.BinaryCrossentropy()\n",
    "    return bce(tvad, pvad) + bce(tvnr, pvnr)\n",
    "\n",
    "paddings = [\n",
    "    [1,0],\n",
    "    [1,1],\n",
    "]\n",
    "\n",
    "\n",
    "def CRNModel(input_shape=(5, 64)):\n",
    "    x = Input(shape=input_shape, name='input')\n",
    "    \n",
    "    y = Reshape((5, 64, 1))(x)\n",
    "    \n",
    "    y = ZeroPadding2D(padding=paddings)(y)\n",
    "    y = Conv2D(16, kernel_size=(2, 3), strides=(1,2), padding='valid')(y)\n",
    "    y = PReLU()(y)\n",
    "    \n",
    "    y = ZeroPadding2D(padding=paddings)(y)\n",
    "    y = Conv2D(32, kernel_size=(2, 3), strides=(1,2), padding='valid')(y)\n",
    "    y = PReLU()(y)\n",
    "    \n",
    "    y = ZeroPadding2D(padding=paddings)(y)\n",
    "    y = Conv2D(64, kernel_size=(2, 3), strides=(1,2), padding='valid')(y)\n",
    "    y = PReLU()(y)\n",
    "    \n",
    "    y = ZeroPadding2D(padding=paddings)(y)\n",
    "    y = Conv2D(128, kernel_size=(2, 3), strides=(1,2), padding='valid')(y)\n",
    "    y = PReLU()(y)\n",
    "    \n",
    "    y = Reshape((5, 512))(y)\n",
    "    \n",
    "    y = GRU(512, activation=\"sigmoid\", recurrent_activation=\"tanh\")(y)\n",
    "    \n",
    "    y = Dense(256)(y)\n",
    "    y = PReLU()(y)\n",
    "\n",
    "    VAD_VNR = Dense(2, activation='sigmoid', name='VAD_VNR')(y)\n",
    "    return Model(inputs=x, outputs=VAD_VNR)\n",
    "\n",
    "\n",
    "adamW = tfa.optimizers.AdamW(learning_rate=_ADAM_W_LR, \n",
    "                             weight_decay=_ADAM_W_WD)\n",
    "                             #gradient_transformers=[AutoClipper(10)])\n",
    "    \n",
    "#bce = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "\n",
    "auc = tf.keras.metrics.AUC(name='AUC', summation_method='minoring')\n",
    "\n",
    "\n",
    "model = CRNModel()\n",
    "model.compile(optimizer=adamW, \n",
    "              loss=lossSUM2BCE,\n",
    "              metrics=auc)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T23:28:27.587041Z",
     "start_time": "2021-05-15T23:28:27.578381Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class DataLoader(Sequence):\n",
    "    def __init__(self, x, y, signal_per_batch, frame_size=5, shuffle=False, *args, **kwargs):\n",
    "        super(DataLoader, self).__init__(*args, **kwargs)\n",
    "        self.x, self.y = np.array(x), np.stack(y, axis=-1)\n",
    "        self.signal_per_batch = signal_per_batch\n",
    "        self.frame_size = frame_size\n",
    "        self.shuffle=shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        total_length = self.data_per_signal * len(self.x)\n",
    "        return math.ceil(total_length / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        indices = self.indices[idx*self.signal_per_batch:(idx+1)*self.signal_per_batch]\n",
    "        signal_batch_x = [self.x[i] for i in indices]\n",
    "        signal_batch_y = [self.y[i] for i in indices]\n",
    "        \n",
    "        batch_x = self._generate_window_x(signal_batch_x)\n",
    "        batch_y = self._generate_window_y(signal_batch_y)\n",
    "        #return np.array(batch_x), np.split(batch_y, 2, axis=-1)\n",
    "        return np.array(batch_x), np.array(batch_y)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(len(self.x))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "    def _generate_window_x(self, batch):\n",
    "        new_batch = []\n",
    "        for signal in batch:\n",
    "            for idx in range(self.data_per_signal):\n",
    "                new_batch.append(signal[idx:idx+self.frame_size])\n",
    "        return np.array(new_batch)\n",
    "    \n",
    "    def _generate_window_y(self, batch):\n",
    "        new_batch = []\n",
    "        for signal in batch:\n",
    "            for idx in range(self.data_per_signal):\n",
    "                new_batch.append(signal[idx+self.frame_size-1])\n",
    "        return np.array(new_batch)\n",
    "            \n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self.signal_per_batch * self.data_per_signal\n",
    "    \n",
    "    @property\n",
    "    def frames_per_signal(self):\n",
    "        return len(self.x[0])\n",
    "    \n",
    "    @property\n",
    "    def data_per_signal(self):\n",
    "        return self.frames_per_signal - self.frame_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T22:51:48.750271Z",
     "start_time": "2021-05-15T22:51:48.549788Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(x=x_train, y=[vad_train, vnr_train], shuffle=True, signal_per_batch=20)\n",
    "\n",
    "valid_loader = DataLoader(x=x_valid, y=[vad_valid, vnr_valid], signal_per_batch=20)\n",
    "\n",
    "test_loader = DataLoader(x=x_test, y=[vad_test, vnr_test], signal_per_batch=10)\n",
    "\n",
    "x, y = train_loader[0]\n",
    "\n",
    "x[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T23:45:21.069005Z",
     "start_time": "2021-05-15T23:39:53.775568Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "model.fit(train_loader, epochs=50, validation_data=valid_loader)\n",
    "\n",
    "print('\\n\\n', '=' * 50, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T23:56:59.555254Z",
     "start_time": "2021-05-15T23:56:59.098961Z"
    }
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_loader, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "866.997px",
    "left": "1665px",
    "right": "20px",
    "top": "127.99px",
    "width": "413.976px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
